# ğŸ§  Ollama Benchmark Tool

Ein flexibles Benchmark-Tool zur Auswertung und Visualisierung der Inferenzleistung lokal gehosteter LLMs mit [Ollama](https://ollama.com/).  
Erzeugt einen vollstÃ¤ndigen HTML-Report mit Tabelle, optionalem Diagramm und Systeminformationen.

---

## ğŸš€ Features

- ğŸ“Š HTML-Report mit Tabelle und (optionalem) Diagramm
- ğŸ§  Tokenrate, Evaluierungsdauer, Ladezeit, GPU-Nutzung & Score
- âš™ï¸ VollstÃ¤ndig parametrierbar via CLI
- âœ… Automatisches Ã–ffnen im Standardbrowser
- ğŸ“¦ Einsetzbar fÃ¼r lokale und entfernte Ollama-Server

---

## âš™ï¸ Verwendung

```bash
python ollama_benchmark.py [OPTIONEN]
```

### Beispiel:

```bash
python ollama_benchmark.py \
  --prompt "Was ist ein Dieselmotor?" \
  --output mein_benchmark \
  --timeout 45 \
  --host http://localhost:11434
```

---

## ğŸ§¾ VerfÃ¼gbare Parameter

| Parameter       | Beschreibung                                         | Standardwert                            |
|----------------|------------------------------------------------------|-----------------------------------------|
| `--prompt`      | Prompt-Text, der an das Modell gesendet wird         | `"Was ist der Unterschied ..."`         |
| `--output`      | Dateiname fÃ¼r den HTML-Report (Zeitstempel wird angehÃ¤ngt) | `ollama_benchmark`               |
| `--no-chart`    | Diagrammausgabe im HTML deaktivieren (optional Flag) | (Diagramm wird standardmÃ¤ÃŸig angezeigt) |
| `--timeout`     | Timeout fÃ¼r den Modellaufruf (in Sekunden)           | `60`                                    |
| `--host`        | Adresse des Ollama-Servers (lokal oder remote)       | `http://localhost:11434`                |

---

## ğŸ“Š HTML-Report enthÃ¤lt:

- âœ… Ãœbersichtstabelle mit:
  - Modellname
  - Anzahl Tokens
  - `eval_duration` (s)
  - `load_duration` (s)
  - Tokenrate (tokens/s)
  - GPU genutzt (âœ…/âŒ)
  - Score (Tokenrate pro Sekunde Rechenzeit)
- ğŸ“ˆ Optionales Balkendiagramm (Chart.js)
- ğŸ–¥ï¸ Systeminformationen:
  - Betriebssystem, Python-Version, GPU, Ollama-Version

---

## ğŸ“‚ Ausgabe

Die HTML-Datei wird im aktuellen Verzeichnis gespeichert und automatisch im Standardbrowser geÃ¶ffnet:

```bash
ollama_benchmark_mein_benchmark_2025-05-14_21-22.html
```

---

## ğŸ“Œ Hinweise

- Das Tool verwendet die Ollama HTTP-API (`/api/generate`), die automatisch durch `ollama run` oder `ollama serve` bereitgestellt wird.
- Die Datei- und Modellnamen werden automatisch mit dem Zeitstempel versehen.
- Das Tool funktioniert unabhÃ¤ngig davon, ob die Modelle CPU- oder GPU-basiert ausgefÃ¼hrt werden.

---

## ğŸ“„ Lizenz

MIT License â€“ nutzbar, erweiterbar, frei einsetzbar.
